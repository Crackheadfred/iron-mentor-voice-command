#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test sp√©cifique pour Ollama
V√©rifie si Ollama fonctionne correctement avec des mod√®les l√©gers
"""

import requests
import json
import sys
import time
import psutil
from pathlib import Path

def check_ollama_service():
    """V√©rifier si le service Ollama est actif"""
    print("=== V√âRIFICATION SERVICE OLLAMA ===")
    
    try:
        response = requests.get("http://localhost:11434/api/version", timeout=5)
        if response.status_code == 200:
            version_info = response.json()
            print(f"‚úÖ Ollama actif - Version: {version_info.get('version', 'Inconnue')}")
            return True
        else:
            print(f"‚ùå Ollama r√©pond avec le code: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("‚ùå Ollama n'est pas accessible sur localhost:11434")
        print("   V√©rifiez qu'Ollama est d√©marr√©")
        return False
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification: {e}")
        return False

def check_available_models():
    """Lister les mod√®les disponibles"""
    print("\n=== MOD√àLES DISPONIBLES ===")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get('models', [])
            if models:
                print(f"‚úÖ {len(models)} mod√®le(s) install√©(s):")
                for model in models:
                    name = model.get('name', 'Nom inconnu')
                    size = model.get('size', 0) / (1024**3)  # Conversion en GB
                    print(f"   - {name} ({size:.1f} GB)")
                return models
            else:
                print("‚ö†Ô∏è  Aucun mod√®le install√©")
                return []
        else:
            print(f"‚ùå Erreur {response.status_code}")
            return []
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return []

def test_lightweight_model():
    """Tester avec un mod√®le l√©ger"""
    print("\n=== TEST MOD√àLE L√âGER ===")
    
    # Mod√®les l√©gers recommand√©s par ordre de pr√©f√©rence
    lightweight_models = [
        "llama3.2:1b",
        "llama3.2:3b", 
        "gemma2:2b",
        "phi3:mini"
    ]
    
    models = check_available_models()
    model_names = [m.get('name', '') for m in models]
    
    # Trouver le premier mod√®le l√©ger disponible
    selected_model = None
    for model in lightweight_models:
        if any(model in name for name in model_names):
            selected_model = model
            break
    
    if not selected_model:
        print("‚ùå Aucun mod√®le l√©ger trouv√©")
        print("Installez un mod√®le l√©ger avec: ollama pull llama3.2:1b")
        return False
    
    print(f"üß™ Test avec le mod√®le: {selected_model}")
    
    try:
        # Test simple avec monitoring m√©moire
        memory_before = psutil.virtual_memory().percent
        print(f"   M√©moire avant: {memory_before:.1f}%")
        
        payload = {
            "model": selected_model,
            "prompt": "Bonjour",
            "stream": False,
            "options": {
                "temperature": 0.1,
                "num_ctx": 1024  # Contexte r√©duit
            }
        }
        
        print("   Envoi de la requ√™te...")
        start_time = time.time()
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            timeout=30
        )
        
        end_time = time.time()
        memory_after = psutil.virtual_memory().percent
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', '')
            
            print(f"‚úÖ Test r√©ussi!")
            print(f"   Temps de r√©ponse: {end_time - start_time:.1f}s")
            print(f"   M√©moire apr√®s: {memory_after:.1f}%")
            print(f"   Augmentation m√©moire: {memory_after - memory_before:.1f}%")
            print(f"   R√©ponse: {response_text[:100]}...")
            
            # Alerte si trop de m√©moire utilis√©e
            if memory_after > 85:
                print("‚ö†Ô∏è  ATTENTION: Utilisation m√©moire √©lev√©e!")
                print("   Consid√©rez un mod√®le encore plus l√©ger")
            
            return True
        else:
            print(f"‚ùå Erreur {response.status_code}: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print("‚ùå Timeout - Le mod√®le met trop de temps √† r√©pondre")
        return False
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

def suggest_configuration():
    """Sugg√©rer une configuration optimale"""
    print("\n=== CONFIGURATION RECOMMAND√âE ===")
    
    # V√©rifier la RAM disponible
    memory = psutil.virtual_memory()
    total_gb = memory.total / (1024**3)
    available_gb = memory.available / (1024**3)
    
    print(f"RAM totale: {total_gb:.1f} GB")
    print(f"RAM disponible: {available_gb:.1f} GB")
    
    if total_gb < 8:
        print("‚ö†Ô∏è  RAM limit√©e - Utilisez llama3.2:1b")
        recommended_model = "llama3.2:1b"
    elif total_gb < 16:
        print("‚úÖ RAM suffisante - Utilisez llama3.2:3b")
        recommended_model = "llama3.2:3b"
    else:
        print("‚úÖ RAM abondante - Vous pouvez utiliser des mod√®les plus gros")
        recommended_model = "llama3.2:3b"
    
    # Mettre √† jour la configuration si n√©cessaire
    config_path = Path('config/config.json')
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            current_model = config.get('ollama', {}).get('model', '')
            if current_model != recommended_model:
                print(f"\nüìù Mise √† jour recommand√©e de la configuration:")
                print(f"   Actuel: {current_model}")
                print(f"   Recommand√©: {recommended_model}")
                
                config['ollama']['model'] = recommended_model
                config['ollama']['enable_low_memory'] = True
                config['ollama']['context_length'] = 2048
                
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                
                print("‚úÖ Configuration mise √† jour!")
        except Exception as e:
            print(f"‚ùå Erreur lors de la mise √† jour: {e}")

def main():
    """Fonction principale"""
    print("JARVIS - TEST OLLAMA")
    print("=" * 40)
    
    success = True
    
    # Tests s√©quentiels
    if not check_ollama_service():
        success = False
        print("\nüîß SOLUTIONS:")
        print("1. D√©marrez Ollama manuellement")
        print("2. V√©rifiez qu'il n'y a pas d'erreur au d√©marrage")
        print("3. Red√©marrez votre ordinateur si n√©cessaire")
        return False
    
    models = check_available_models()
    if not models:
        success = False
        print("\nüîß SOLUTION:")
        print("Installez un mod√®le l√©ger: ollama pull llama3.2:1b")
        return False
    
    if not test_lightweight_model():
        success = False
    
    suggest_configuration()
    
    if success:
        print("\nüéâ OLLAMA FONCTIONNE CORRECTEMENT!")
    else:
        print("\n‚ö†Ô∏è  PROBL√àMES D√âTECT√âS AVEC OLLAMA")
    
    return success

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nTest interrompu par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• ERREUR CRITIQUE: {e}")
        sys.exit(1)