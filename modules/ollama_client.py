#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Client Ollama pour J.A.R.V.I.S.
Communication avec le mod√®le Mistral local
"""

import requests
import json
import logging
from datetime import datetime
import time
import torch

class OllamaClient:
    def __init__(self, config):
        """Initialisation du client Ollama"""
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        self.base_url = config["ollama"]["url"]
        self.model = config["ollama"]["model"]
        
        # Personnalit√© de J.A.R.V.I.S.
        self.system_prompt = """Tu es J.A.R.V.I.S., l'assistant personnel intelligent d'Iron Man.
        Tu es poli, professionnel, l√©g√®rement sarcastique parfois, et tr√®s comp√©tent.
        Tu t'adresses √† ton utilisateur avec respect ("Monsieur" ou "Madame").
        Tu es capable d'analyser des situations complexes et de fournir des conseils techniques.
        Tu peux aider avec la programmation, les jeux de simulation, l'aviation, et bien d'autres sujets.
        R√©ponds toujours en fran√ßais, de mani√®re concise mais compl√®te.
        Si tu n'es pas s√ªr de quelque chose, dis-le clairement."""
        
        self.conversation_history = []
        self.max_history = 20
        
        # Test de connexion
        self.test_connection()
        
    def test_connection(self):
        """Tester la connexion √† Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json()
                model_names = [model["name"] for model in models.get("models", [])]
                
                if self.model in model_names:
                    self.logger.info(f"Connexion Ollama r√©ussie. Mod√®le {self.model} disponible.")
                    return True
                else:
                    self.logger.warning(f"Mod√®le {self.model} non trouv√©. Mod√®les disponibles: {model_names}")
            else:
                self.logger.error(f"Erreur Ollama: {response.status_code}")
                
        except requests.exceptions.ConnectionError:
            self.logger.error("Impossible de se connecter √† Ollama. V√©rifiez qu'Ollama est d√©marr√©.")
        except Exception as e:
            self.logger.error(f"Erreur lors du test de connexion Ollama: {e}")
            
        return False
    
    def get_response(self, user_input, context=None):
        """Obtenir une r√©ponse du mod√®le Mistral"""
        try:
            print(f"üîÑ D√©but traitement Ollama...")
            
            # Test de connexion rapide
            if not self.test_connection():
                print("‚ùå Connexion Ollama √©chou√©e")
                return "D√©sol√© monsieur, je ne parviens pas √† acc√©der √† mon processeur principal."
            
            # Pr√©parer le prompt avec contexte
            full_prompt = self.prepare_prompt(user_input, context)
            print(f"üìù Prompt pr√©par√©: {full_prompt[:100]}...")
            
            # Pr√©parer la requ√™te
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": self.system_prompt},
                    *self.conversation_history[-self.max_history:],
                    {"role": "user", "content": full_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 300,  # R√©duction pour vitesse
                    "num_predict": 300,
                    "repeat_penalty": 1.1,
                    "num_ctx": 2048,    # Contexte plus petit = plus rapide
                    "num_gpu": -1 if torch.cuda.is_available() else 0  # Utiliser GPU si disponible
                }
            }
            
            print(f"üöÄ Envoi requ√™te √† Ollama...")
            
            # Envoi de la requ√™te
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=30
            )
            
            print(f"üì® R√©ponse re√ßue - Status: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                assistant_response = result["message"]["content"]
                
                print(f"‚úÖ R√©ponse obtenue: {assistant_response[:100]}...")
                
                # Ajouter √† l'historique
                self.conversation_history.append({"role": "user", "content": user_input})
                self.conversation_history.append({"role": "assistant", "content": assistant_response})
                
                # Limiter l'historique
                if len(self.conversation_history) > self.max_history * 2:
                    self.conversation_history = self.conversation_history[-self.max_history:]
                
                self.logger.info(f"R√©ponse Ollama obtenue: {assistant_response[:100]}...")
                return assistant_response
                
            else:
                error_msg = f"Erreur HTTP {response.status_code}: {response.text}"
                print(f"‚ùå {error_msg}")
                self.logger.error(error_msg)
                return "D√©sol√©, je rencontre des difficult√©s techniques avec mon processeur principal."
                
        except requests.exceptions.Timeout:
            error_msg = "Timeout lors de la requ√™te Ollama"
            print(f"‚è∞ {error_msg}")
            self.logger.error(error_msg)
            return "D√©sol√© monsieur, le temps de traitement a √©t√© d√©pass√©."
            
        except requests.exceptions.ConnectionError:
            error_msg = "Impossible de se connecter √† Ollama"
            print(f"üîå {error_msg}")
            self.logger.error(error_msg)
            return "Je ne parviens pas √† me connecter √† Ollama. V√©rifiez qu'il est d√©marr√©."
            
        except Exception as e:
            error_msg = f"Erreur lors de la requ√™te Ollama: {e}"
            print(f"üí• {error_msg}")
            self.logger.error(error_msg)
            return "Je rencontre une erreur technique. Veuillez r√©essayer."
    
    def prepare_prompt(self, user_input, context=None):
        """Pr√©parer le prompt avec le contexte"""
        prompt = user_input
        
        if context:
            context_info = []
            
            # Ajouter l'heure
            context_info.append(f"Heure actuelle: {datetime.now().strftime('%H:%M:%S')}")
            
            # Ajouter les donn√©es d'√©cran si disponibles
            if context.get("screen_data"):
                context_info.append(f"Contenu de l'√©cran: {context['screen_data']}")
            
            # Ajouter les donn√©es de t√©l√©m√©trie si disponibles
            if context.get("telemetry"):
                telemetry = context["telemetry"]
                context_info.append(f"T√©l√©m√©trie course: Vitesse: {telemetry.get('speed', 'N/A')} km/h")
            
            # Ajouter l'√©tat des modules
            if context.get("dcs_active"):
                context_info.append("Module DCS F/A-18 actif")
            
            if context_info:
                prompt = f"[CONTEXTE: {' | '.join(context_info)}]\n\nQuestion: {user_input}"
        
        return prompt
    
    def get_models(self):
        """Obtenir la liste des mod√®les disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json()
                return [model["name"] for model in models.get("models", [])]
        except Exception as e:
            self.logger.error(f"Erreur lors de la r√©cup√©ration des mod√®les: {e}")
        
        return []
    
    def switch_model(self, model_name):
        """Changer de mod√®le"""
        available_models = self.get_models()
        if model_name in available_models:
            self.model = model_name
            self.logger.info(f"Mod√®le chang√© vers: {model_name}")
            return True
        else:
            self.logger.warning(f"Mod√®le {model_name} non disponible")
            return False
    
    def clear_history(self):
        """Effacer l'historique de conversation"""
        self.conversation_history = []
        self.logger.info("Historique de conversation effac√©")
    
    def get_status(self):
        """Obtenir le statut du client"""
        return {
            "connected": self.test_connection(),
            "model": self.model,
            "base_url": self.base_url,
            "history_length": len(self.conversation_history),
            "available_models": self.get_models()
        }